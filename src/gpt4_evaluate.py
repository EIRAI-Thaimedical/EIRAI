import argparse
import json
import os
import openai

# Set up OpenAI API key from environment variables
openai.api_key = os.getenv("API_KEY")

def generate_instruction_prompt(row, key):
    """
    Generates a detailed prompt for evaluating AI model responses based on clinical notes and questions.

    Args:
        row (dict): A dictionary containing input data.
        key (str): The key for accessing the specific clinical note and question.

    Returns:
        str: A formatted string that will be sent to the GPT model for evaluation.
    """
    clinical_note = row["input_thai"][key]
    question = row["instruction_thai"][key]

    # Construct the prompt
    prompt = f"""
    You are a helpful and precise Healthcare assistant for checking the quality of the answer.
    [The start of Clinical note]
    {clinical_note}
    [The end of Clinical note]
    [The start of Question]
    {question}
    [The end of Question]
    """

    # Add assistant answers to the prompt
    for i, model_key in enumerate(row["scores"]):
        prompt += f"""
        [The Start of Assistant {i+1}'s Answer]
        {row["models"][model_key][key]}
        [The End of Assistant {i+1}'s Answer]
        """
    
    # Request feedback for AI assistant performance evaluation
    prompt += """
    Please rate the relevance and accuracy of their responses. Each assistant receives an overall score on a scale of 1 to 10, 
    where a higher score indicates better overall performance.
    Output a single line with scores for each assistant, separated by a space. In the next line, provide a brief explanation 
    of your evaluation, ensuring objectivity.
    """

    return prompt

def make_answer_gpt(message):
    """
    Sends a prompt to the GPT model and returns the generated response.

    Args:
        message (str): The prompt to send to the GPT model.

    Returns:
        str: The response generated by the GPT model.
    """
    try:
        response = openai.chat.completions.create(
            model="gpt-4o",
            temperature=0.3,
            messages=[{"role": "user", "content": message}]
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        print(f"Error generating response: {e}")
        return None

def attempt_scoring(question, retries, mtsamples):
    """
    Attempts to get valid scores from the GPT model within a set number of retries.

    Args:
        question (str): The prompt to send to the GPT model.
        retries (int): Number of times to attempt generating a valid response.
        mtsamples (dict): Dataset containing the name of models.

    Returns:
        tuple: A tuple containing the generated answer and a list of split scores.
    """
    count = 0
    while count < retries:
        answers = make_answer_gpt(question)
        if not answers:
            count += 1
            continue
        
        answers = answers.strip().strip("\"").strip("\'")
        split_answer = answers.split()

        if all(score.isdigit() for score in split_answer[:len(mtsamples["scores"])]):
            return answers, split_answer
        
        count += 1
    
    return "Failed to generate valid scores", [0] * len(mtsamples["scores"])

def main(models: list, score_path: str, explain_path: str):
    """
    Main function that evaluates model responses and saves the results to JSON files.

    Args:
        models (list): List of models to evaluate. If None, all models are evaluated.
        score_path (str): File path where the score results will be saved.
        explain_path (str): File path where the explanation results will be saved.
    """
    mtsamples = json.load(open("dataset/EHR_task_responses.json", "r", encoding="utf-8"))
    mtsamples["scores"] = {}
    explanation_samples = {}

    # Initialize scores for all models
    if not models:
        for key in mtsamples["models"]:
            mtsamples["scores"][key] = {}
    else:
        mtsamples["scores"]['gpt-3.5'] = {}
        mtsamples["scores"]['gpt-4o'] = {}
        for model in models:
            if model not in mtsamples["models"]:
                raise ValueError(f"Model {model} not found in the dataset.")
            mtsamples["scores"][model] = {}

    # Loop through all tasks and evaluate model responses
    for i, key in enumerate(mtsamples["type"]):
        question_message = generate_instruction_prompt(mtsamples, key)
        answers, split_answer = attempt_scoring(question_message, 5, mtsamples)

        # Save scores for each model
        for j, model_key in enumerate(mtsamples["scores"]):
            mtsamples["scores"][model_key][key] = int(split_answer[j])
        explanation_samples[key] = answers

        print(f"Processed: {i + 1}/{len(mtsamples['type'])}")

    # Save the scores and explanations to JSON files
    with open(score_path, "w", encoding="utf-8") as score_file:
        json.dump(mtsamples, score_file, ensure_ascii=False, indent=4)
    
    with open(explain_path, "w", encoding="utf-8") as explain_file:
        json.dump(explanation_samples, explain_file, ensure_ascii=False, indent=4)

    print(f"Scores saved to {score_path}")
    print(f"Explanations saved to {explain_path}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Evaluate GPT-4o, GPT-3.5, and other models on healthcare tasks.")
    parser.add_argument("--score-path", type=str, default="dataset/EHR_task_result_scores.json", help="Path to output the score JSON file.")
    parser.add_argument("--explain-path", type=str, default="dataset/EHR_task_result_explain.json", help="Path to output the explanations JSON file.")
    parser.add_argument("--models", nargs='+', default=None, help="Specify models to evaluate. If not set, evaluates all models.")
    args = parser.parse_args()

    main(models=args.models, score_path=args.score_path, explain_path=args.explain_path)
